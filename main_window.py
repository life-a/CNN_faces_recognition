import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import cv2
import dlib
import numpy as np
import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()  # ç¦ç”¨TensorFlow 2.xè¡Œä¸º
import os
import time
from PIL import Image, ImageTk
import glob
from collections import deque, Counter
import random
from datetime import datetime
import threading
import shutil


def layer_net(input_image, num_class, dropout_rate, dropout_rate_2):
    """å®Œå…¨æŒ‰ç…§åŸä»£ç å®šä¹‰"""
    tf.disable_eager_execution()

    """ç¬¬ä¸€ã€äºŒå±‚ï¼Œè¾“å…¥å›¾ç‰‡64*64*3ï¼Œè¾“å‡ºå›¾ç‰‡32*32*32"""
    w1 = tf.Variable(tf.random.normal([3, 3, 3, 32]), name='w1')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(3)ï¼Œ è¾“å‡ºé€šé“(32)
    b1 = tf.Variable(tf.random.normal([32]), name='b1')
    layer_conv1 = tf.nn.relu(
        tf.nn.conv2d(input_image, w1, strides=[1, 1, 1, 1], padding='SAME') + b1)  # 64*64*32ï¼Œå·ç§¯æå–ç‰¹å¾ï¼Œå¢åŠ é€šé“æ•°
    layer_pool1 = tf.nn.max_pool2d(layer_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                                   padding='SAME')  # 32*32*32ï¼Œæ± åŒ–é™ç»´ï¼Œå‡å°å¤æ‚åº¦
    drop1 = tf.nn.dropout(layer_pool1, rate=1 - dropout_rate)  # æŒ‰ä¸€å®šæ¦‚ç‡éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼Œä»¥è·å¾—æ›´é«˜çš„è®­ç»ƒé€Ÿåº¦ä»¥åŠé˜²æ­¢è¿‡æ‹Ÿåˆ

    """ç¬¬ä¸‰ã€å››å±‚ï¼Œè¾“å…¥å›¾ç‰‡32*32*32ï¼Œè¾“å‡ºå›¾ç‰‡16*16*64"""
    w2 = tf.Variable(tf.random.normal([3, 3, 32, 64]), name='w2')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(32)ï¼Œ è¾“å‡ºé€šé“(64)
    b2 = tf.Variable(tf.random.normal([64]), name='b2')
    layer_conv2 = tf.nn.relu(tf.nn.conv2d(drop1, w2, strides=[1, 1, 1, 1], padding='SAME') + b2)  # 32*32*64
    layer_pool2 = tf.nn.max_pool2d(layer_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 16*16*64
    drop2 = tf.nn.dropout(layer_pool2, rate=1 - dropout_rate)

    """ç¬¬äº”ã€å…­å±‚ï¼Œè¾“å…¥å›¾ç‰‡16*16*64ï¼Œè¾“å‡ºå›¾ç‰‡8*8*64"""
    w3 = tf.Variable(tf.random.normal([3, 3, 64, 64]), name='w3')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(64)ï¼Œ è¾“å‡ºé€šé“(64)
    b3 = tf.Variable(tf.random.normal([64]), name='b3')
    layer_conv3 = tf.nn.relu(tf.nn.conv2d(drop2, w3, strides=[1, 1, 1, 1], padding='SAME') + b3)  # 16*16*64
    layer_pool3 = tf.nn.max_pool2d(layer_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                                   padding='SAME')  # 8*8*64=4096
    drop3 = tf.nn.dropout(layer_pool3, rate=1 - dropout_rate)

    """ç¬¬ä¸ƒå±‚ï¼Œå…¨è¿æ¥å±‚ï¼Œå°†å›¾ç‰‡çš„å·ç§¯è¾“å‡ºå‹æ‰æˆä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œè¾“å…¥å›¾ç‰‡8*8*64ï¼Œreshapeåˆ°1*4096ï¼Œè¾“å‡º1*512"""
    w4 = tf.Variable(tf.random.normal([8 * 8 * 64, 512]), name='w4')  # è¾“å…¥é€šé“(4096)ï¼Œ è¾“å‡ºé€šé“(512)
    b4 = tf.Variable(tf.random.normal([512]), name='b4')
    layer_fully_connected = tf.reshape(drop3, [-1, 8 * 8 * 64])  # -1è¡¨ç¤ºè¡Œéšç€åˆ—çš„éœ€æ±‚æ”¹å˜ï¼Œ1*4096
    relu = tf.nn.relu(tf.matmul(layer_fully_connected, w4) + b4)  # [1,4096]*[4096,512]=[1,512]
    drop4 = tf.nn.dropout(relu, rate=1 - dropout_rate_2)

    """ç¬¬å…«å±‚ï¼Œè¾“å‡ºå±‚ï¼Œè¾“å…¥1*512ï¼Œè¾“å‡º1*2ï¼Œå†add"""
    w5 = tf.Variable(tf.random.normal([512, num_class]), name='w5')  # è¾“å…¥é€šé“(512)ï¼Œ è¾“å‡ºé€šé“(2)
    b5 = tf.Variable(tf.random.normal([num_class]), name='b5')
    outdata = tf.add(tf.matmul(drop4, w5), b5)  # (1,512)*(512,2)=(1,2) ,è·Ÿinput_label [0,1]ã€[1,0]æ¯”è¾ƒç»™å‡ºæŸå¤± ï¼Œå…ˆä¹˜å†åŠ 
    return outdata


class FaceDataCollector:
    """äººè„¸æ•°æ®é‡‡é›†å™¨ï¼ˆæ ¸å¿ƒåŠŸèƒ½ç‰ˆï¼‰"""

    def __init__(self, size=64):
        self.size = size
        self.detector = dlib.get_frontal_face_detector()

    def apply_augmentations(self, img):
        """åº”ç”¨ä¸‰ç§æ•°æ®å¢å¼º"""
        augmentations = []

        # 1. æ°´å¹³ç¿»è½¬
        flipped = cv2.flip(img, 1)
        augmentations.append(flipped)

        # 2. äº®åº¦è°ƒæ•´ï¼ˆéšæœºå˜äº®æˆ–å˜æš—ï¼‰
        alpha = random.uniform(0.7, 1.3)  # å¯¹æ¯”åº¦
        beta = random.randint(-30, 30)  # äº®åº¦
        bright = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)
        augmentations.append(bright)

        # 3. å¯¹æ¯”åº¦è°ƒæ•´
        contrast = cv2.convertScaleAbs(img, alpha=random.uniform(0.8, 1.2), beta=0)
        augmentations.append(contrast)

        return augmentations

    def capture_data(self, person_name, target_count=100, cap=None):
        """
        é‡‡é›†æŒ‡å®šäººå‘˜çš„äººè„¸æ•°æ®
        :param person_name: äººå‘˜åç§°ï¼ˆè‹±æ–‡æˆ–æ‹¼éŸ³ï¼Œä¸è¦ç”¨ä¸­æ–‡ï¼‰
        :param target_count: ç›®æ ‡é‡‡é›†æ•°é‡ï¼ˆåŸå§‹+å¢å¼ºåçš„æ€»æ•°é‡ï¼‰
        :param cap: æ‘„åƒå¤´å¯¹è±¡
        """
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼šfaces_ok/äººå‘˜åç§°/
        save_dir = os.path.join('./faces_ok', person_name)

        # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ‰€æœ‰å›¾ç‰‡
        if os.path.exists(save_dir):
            for file in os.listdir(save_dir):
                file_path = os.path.join(save_dir, file)
                if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.png', '.jpeg')):
                    os.remove(file_path)
            print(f"å·²æ¸…ç©º {person_name} çš„æ—§ç…§ç‰‡")
        else:
            os.makedirs(save_dir, exist_ok=True)

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹é‡‡é›† [{person_name}] çš„äººè„¸æ•°æ®")
        print(f"ç›®æ ‡æ•°é‡: {target_count}å¼ ï¼ˆå«3å€å¢å¼ºï¼‰")
        print(f"ä¿å­˜ç›®å½•: {save_dir}")
        print(f"{'=' * 60}")

        # æ£€æŸ¥å·²æœ‰å›¾ç‰‡æ•°é‡ï¼ˆåº”è¯¥ä¸º0ï¼‰
        existing_files = [f for f in os.listdir(save_dir) if f.endswith(('.jpg', '.png'))]
        saved_count = len(existing_files)
        print(f"ğŸ“ å½“å‰ç›®å½•å›¾ç‰‡æ•°: {saved_count} å¼ ")

        frame_skip = 1  # æ¯1å¸§é‡‡é›†ä¸€æ¬¡ï¼Œæé«˜é‡‡é›†é€Ÿåº¦
        frame_counter = 0

        return target_count, save_dir, frame_skip, frame_counter


def train_model():
    """è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ï¼ˆä»faces_train_multi_person.pyæå–ï¼‰"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    faces_ok_dir = './faces_ok'
    if not os.path.exists(faces_ok_dir):
        print("faces_okç›®å½•ä¸å­˜åœ¨")
        return False

    class_names = [d for d in os.listdir(faces_ok_dir) if os.path.isdir(os.path.join(faces_ok_dir, d))]
    if len(class_names) < 2:
        print("æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2ä¸ªç±»åˆ«")
        return False

    # æ·»åŠ é™Œç”Ÿäººç±»åˆ«
    class_names.append("é™Œç”Ÿäºº")
    num_classes = len(class_names)

    print(f"æ£€æµ‹åˆ° {len(class_names) - 1} ä¸ªäººè„¸ç±»åˆ«: {class_names[:-1]}")

    # è¯»å–æ•°æ®
    def read_data():
        images = []
        labels = []

        for i, class_name in enumerate(class_names):
            if class_name == "é™Œç”Ÿäºº":
                continue  # é™Œç”Ÿäººæ˜¯è™šæ‹Ÿç±»åˆ«ï¼Œä¸è¯»å–å®é™…å›¾ç‰‡

            class_dir = os.path.join(faces_ok_dir, class_name)
            for img_file in os.listdir(class_dir):
                if img_file.lower().endswith(('.jpg', '.png', '.jpeg')):
                    img_path = os.path.join(class_dir, img_file)
                    img = cv2.imread(img_path)
                    if img is not None:
                        img = cv2.resize(img, (64, 64))
                        img = img.astype(np.float32) / 255.0
                        images.append(img)

                        # åˆ›å»ºone-hotæ ‡ç­¾
                        label = [0] * num_classes
                        label[i] = 1
                        labels.append(label)

        return np.array(images), np.array(labels)

    try:
        # æ¸…ç©ºæ¨¡å‹ç›®å½•
        model_dir = './model_multi_class'
        if os.path.exists(model_dir):
            shutil.rmtree(model_dir)
        os.makedirs(model_dir, exist_ok=True)

        # è¯»å–æ•°æ®
        X, y = read_data()
        print(f"è¯»å–åˆ° {len(X)} å¼ å›¾ç‰‡")

        if len(X) == 0:
            print("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•å›¾ç‰‡æ•°æ®")
            return False

        # æ£€æŸ¥æ•°æ®æ˜¯å¦å¹³è¡¡
        class_counts = []
        for i in range(num_classes - 1):  # ä¸åŒ…æ‹¬é™Œç”Ÿäºº
            count = sum(1 for label in y if np.argmax(label) == i)
            class_counts.append(count)
            print(f"ç±»åˆ« {class_names[i]}: {count} å¼ å›¾ç‰‡")

        if min(class_counts) == 0:
            print("æŸäº›ç±»åˆ«æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•è®­ç»ƒ")
            return False

        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]

        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        split_idx = int(0.8 * len(X))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        print(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

        # å®šä¹‰æ¨¡å‹
        input_image = tf.placeholder(tf.float32, [None, 64, 64, 3], name='input_image')
        input_label = tf.placeholder(tf.float32, [None, num_classes], name='input_label')
        dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
        dropout_rate_2 = tf.placeholder(tf.float32, name='dropout_rate_2')

        outdata = layer_net(input_image, num_classes, dropout_rate, dropout_rate_2)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_label, logits=outdata))
        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)

        # å®šä¹‰å‡†ç¡®ç‡
        correct_prediction = tf.equal(tf.argmax(outdata, 1), tf.argmax(input_label, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        # ä¿å­˜æ¨¡å‹
        saver = tf.train.Saver()

        # åˆ›å»ºä¼šè¯
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)
        sess.run(tf.global_variables_initializer())

        # è®­ç»ƒæ¨¡å‹
        batch_size = 32
        epochs = 50  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦

        best_accuracy = 0
        patience = 10
        patience_counter = 0

        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        for epoch in range(epochs):
            # è®¡ç®—æ‰¹æ¬¡æ•°é‡
            num_batches = len(X_train) // batch_size

            # è®­ç»ƒ
            total_loss = 0
            for i in range(num_batches):
                start_idx = i * batch_size
                end_idx = start_idx + batch_size

                batch_x = X_train[start_idx:end_idx]
                batch_y = y_train[start_idx:end_idx]

                _, loss_val = sess.run([optimizer, loss],
                                       feed_dict={
                                           input_image: batch_x,
                                           input_label: batch_y,
                                           dropout_rate: 0.5,
                                           dropout_rate_2: 0.5
                                       })
                total_loss += loss_val

            # æµ‹è¯•å‡†ç¡®ç‡
            if len(X_test) > 0:
                test_accuracy = sess.run(accuracy,
                                         feed_dict={
                                             input_image: X_test,
                                             input_label: y_test,
                                             dropout_rate: 1.0,
                                             dropout_rate_2: 1.0
                                         })

                print(
                    f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / num_batches:.4f}, Test Accuracy: {test_accuracy:.4f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if test_accuracy > best_accuracy:
                    best_accuracy = test_accuracy
                    model_path = f'./model_multi_class/best_model-{epoch + 1}'
                    saver.save(sess, model_path)
                    print(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    print("æ—©åœï¼Œè®­ç»ƒç»“æŸ")
                    break
            else:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / num_batches:.4f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = './model_multi_class/final_model'
        saver.save(sess, final_model_path)

        # ä¿å­˜ç±»åˆ«åç§°
        with open('./model_multi_class/class_names.txt', 'w', encoding='utf-8') as f:
            for name in class_names:
                f.write(name + '\n')

        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
        sess.close()
        return True

    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


class ImprovedFaceRecognizer:
    """æ”¹è¿›çš„äººè„¸è¯†åˆ«å™¨ï¼Œè§£å†³æ€»æ˜¯è¯†åˆ«ä¸ºåŒä¸€ä¸ªäººçš„é—®é¢˜"""

    def __init__(self, model_path='./model_multi_class/'):
        self.model_path = model_path
        self.sess = None
        self.outdata = None
        self.input_image = None
        self.dropout_rate = None
        self.dropout_rate_2 = None
        self.class_names = []
        self.num_classes = 0

        # æ—¶é—´å¹³æ»‘å‚æ•°
        self.prediction_history = deque(maxlen=15)  # ä¿å­˜æœ€è¿‘15æ¬¡é¢„æµ‹
        self.confidence_history = deque(maxlen=15)  # ä¿å­˜æœ€è¿‘15æ¬¡ç½®ä¿¡åº¦

        # åŠ¨æ€é˜ˆå€¼å‚æ•°
        self.base_threshold = 0.65  # åŸºç¡€ç½®ä¿¡åº¦é˜ˆå€¼
        self.class_thresholds = {}  # æ¯ä¸ªç±»åˆ«çš„åŠ¨æ€é˜ˆå€¼

    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„å¹³è¡¡æ¨¡å‹ - ä¿®å¤ç‰ˆ"""
        print(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æ¨¡å‹...")

        # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.model_path):
            print(f"é”™è¯¯: æ¨¡å‹ç›®å½• {self.model_path} ä¸å­˜åœ¨")
            return False

        # è¯»å–ç±»åˆ«åç§°
        class_names_file = './model_multi_class/class_names.txt'
        if os.path.exists(class_names_file):
            with open(class_names_file, 'r', encoding='utf-8') as f:
                self.class_names = [line.strip() for line in f.readlines() if line.strip()]
        else:
            # å°è¯•ä»faces_okç›®å½•æ¨æ–­
            faces_ok_dir = './faces_ok'
            if os.path.exists(faces_ok_dir):
                class_names = []
                for item in os.listdir(faces_ok_dir):
                    item_path = os.path.join(faces_ok_dir, item)
                    if os.path.isdir(item_path):
                        class_names.append(item)
                # æ·»åŠ é™Œç”Ÿäººç±»åˆ«
                class_names.append("é™Œç”Ÿäºº")
                self.class_names = class_names
            else:
                print("faces_okç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«")
                self.class_names = ["æˆ‘çš„äººè„¸", "å…¶ä»–äººè„¸"]

        self.num_classes = len(self.class_names)
        print(f"åŠ è½½äº† {self.num_classes} ä¸ªç±»åˆ«: {self.class_names}")

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆå§‹åŒ–åŠ¨æ€é˜ˆå€¼
        for i, name in enumerate(self.class_names):
            if name == "é™Œç”Ÿäºº":
                self.class_thresholds[i] = 0.55  # é™Œç”Ÿäººé˜ˆå€¼è¾ƒä½
            else:
                self.class_thresholds[i] = 0.65  # å·²çŸ¥äººå‘˜é˜ˆå€¼è¾ƒé«˜

        # ç¡®ä¿ä½¿ç”¨TensorFlow 1.xå…¼å®¹æ¨¡å¼
        tf.disable_eager_execution()

        # æŸ¥æ‰¾æ¨¡å‹checkpoint
        checkpoint_path = self.find_latest_model()
        if checkpoint_path is None or checkpoint_path == "":
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹checkpointè·¯å¾„")
            return False

        print(f"æ‰¾åˆ°æ¨¡å‹checkpoint: {checkpoint_path}")

        # æ£€æŸ¥checkpointæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        checkpoint_files = [
            checkpoint_path + '.meta',
            checkpoint_path + '.index',
            checkpoint_path + '.data-00000-of-00001'
        ]

        for file_path in checkpoint_files:
            if not os.path.exists(file_path):
                print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        try:
            # å…³é—­ç°æœ‰ä¼šè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.sess is not None:
                try:
                    self.sess.close()
                except:
                    pass
                self.sess = None

            # é‡ç½®TensorFlowå›¾
            tf.reset_default_graph()

            # å®šä¹‰å ä½ç¬¦
            size = 64
            self.input_image = tf.placeholder(tf.float32, [None, size, size, 3], name='input_image')
            self.dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
            self.dropout_rate_2 = tf.placeholder(tf.float32, name='dropout_rate_2')

            # æ„å»ºç½‘ç»œ
            self.outdata = layer_net(self.input_image, self.num_classes,
                                     self.dropout_rate, self.dropout_rate_2)

            # åˆ›å»ºä¼šè¯
            config = tf.ConfigProto()
            config.gpu_options.allow_growth = True
            self.sess = tf.Session(config=config)

            # åˆ›å»ºSaverå¹¶æ¢å¤æ¨¡å‹
            saver = tf.train.Saver()
            print(f"æ­£åœ¨æ¢å¤æ¨¡å‹: {checkpoint_path}")
            saver.restore(self.sess, checkpoint_path)

            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # å¿«é€Ÿæµ‹è¯•æ¨¡å‹
            test_input = np.random.randn(1, 64, 64, 3) * 0.1
            probs = self.sess.run(tf.nn.softmax(self.outdata),
                                  feed_dict={
                                      self.input_image: test_input,
                                      self.dropout_rate: 1.0,
                                      self.dropout_rate_2: 1.0
                                  })
            print(f"æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºshape: {probs.shape}")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            # å°è¯•å¦ä¸€ç§æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
            print("å°è¯•æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶...")
            model_files = glob.glob(os.path.join(self.model_path, '*.meta'))
            if model_files:
                print(f"æ‰¾åˆ°ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶: {model_files}")
                # å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¨¡å‹
                for model_file in model_files:
                    try:
                        model_path = model_file.replace('.meta', '')
                        print(f"å°è¯•åŠ è½½: {model_path}")

                        # é‡æ–°åˆ›å»ºä¼šè¯å’Œå›¾
                        tf.reset_default_graph()
                        self.sess = tf.Session(config=config)

                        # é‡æ–°å®šä¹‰ç½‘ç»œç»“æ„
                        self.input_image = tf.placeholder(tf.float32, [None, size, size, 3], name='input_image')
                        self.dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
                        self.dropout_rate_2 = tf.placeholder(tf.float32, name='dropout_rate_2')
                        self.outdata = layer_net(self.input_image, self.num_classes,
                                                 self.dropout_rate, self.dropout_rate_2)

                        saver = tf.train.Saver()
                        saver.restore(self.sess, model_path)
                        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
                        return True
                    except Exception as inner_e:
                        print(f"åŠ è½½å¤±è´¥: {inner_e}")

            return False

    def find_latest_model(self):
        """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
        model_dir = './model_multi_class/'
        if not os.path.exists(model_dir):
            print(f"æ¨¡å‹ç›®å½• {model_dir} ä¸å­˜åœ¨")
            return None

        print(f"æœç´¢æ¨¡å‹ç›®å½•: {model_dir}")

        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        all_files = os.listdir(model_dir)
        print(f"ç›®å½•ä¸­çš„æ–‡ä»¶: {all_files}")

        # ä¼˜å…ˆæ£€æŸ¥checkpointæ–‡ä»¶
        checkpoint_file = os.path.join(model_dir, 'checkpoint')
        if os.path.exists(checkpoint_file):
            print("æ‰¾åˆ°checkpointæ–‡ä»¶")
            try:
                with open(checkpoint_file, 'r') as f:
                    content = f.read()
                    print(f"checkpointå†…å®¹:\n{content}")

                    # è§£æcheckpointæ–‡ä»¶
                    for line in content.split('\n'):
                        if 'model_checkpoint_path' in line and ':' in line:
                            # æå–æ¨¡å‹åç§°ï¼Œæ ¼å¼å¦‚: model_checkpoint_path: "best_model-50"
                            model_name = line.split(':')[1].strip().strip('"')
                            model_path = os.path.join(model_dir, model_name)
                            print(f"ä»checkpointè§£æå‡ºçš„æ¨¡å‹è·¯å¾„: {model_path}")

                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                            if os.path.exists(model_path + '.meta'):
                                print(f"æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                                return model_path
                            else:
                                print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ {model_path}.meta ä¸å­˜åœ¨")
            except Exception as e:
                print(f"è¯»å–checkpointæ–‡ä»¶å¤±è´¥: {e}")

        # å¦‚æœcheckpointä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼ŒæŸ¥æ‰¾best_model
        best_model_patterns = [
            os.path.join(model_dir, 'best_model-*'),  # æ—§æ ¼å¼
            os.path.join(model_dir, 'best_model')  # æ–°æ ¼å¼
        ]

        for pattern in best_model_patterns:
            model_files = glob.glob(pattern + '.meta')
            if model_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                model_files.sort(key=os.path.getmtime, reverse=True)
                latest_model = model_files[0]
                model_path = latest_model.replace('.meta', '')
                print(f"æ‰¾åˆ°best_model: {model_path}")

                # æ£€æŸ¥å…¶ä»–å¿…è¦çš„æ–‡ä»¶
                required_files = [model_path + ext for ext in ['.meta', '.index', '.data-00000-of-00001']]
                missing_files = [f for f in required_files if not os.path.exists(f)]

                if missing_files:
                    print(f"è­¦å‘Š: ç¼ºå°‘æ–‡ä»¶: {missing_files}")
                else:
                    return model_path

        # æŸ¥æ‰¾final_model
        final_model_path = os.path.join(model_dir, 'final_model')
        if os.path.exists(final_model_path + '.meta'):
            print(f"æ‰¾åˆ°final_model: {final_model_path}")

            # æ£€æŸ¥å…¶ä»–å¿…è¦çš„æ–‡ä»¶
            required_files = [final_model_path + ext for ext in ['.meta', '.index', '.data-00000-of-00001']]
            missing_files = [f for f in required_files if not os.path.exists(f)]

            if missing_files:
                print(f"è­¦å‘Š: ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            else:
                return final_model_path

        # æŸ¥æ‰¾ä»»ä½•ä»¥.metaç»“å°¾çš„æ–‡ä»¶
        all_meta_files = glob.glob(os.path.join(model_dir, '*.meta'))
        if all_meta_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            all_meta_files.sort(key=os.path.getmtime, reverse=True)
            latest_meta = all_meta_files[0]
            model_path = latest_meta.replace('.meta', '')
            print(f"æ‰¾åˆ°æœ€è¿‘çš„metaæ–‡ä»¶: {model_path}")
            return model_path

        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
        return None

    def preprocess_face(self, face_img):
        """é¢„å¤„ç†äººè„¸å›¾åƒ"""
        if face_img is None or face_img.size == 0:
            return None

        # è°ƒæ•´å¤§å°åˆ°64x64
        face_resized = cv2.resize(face_img, (64, 64))

        # ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆå¢å¼ºå¯¹æ¯”åº¦ï¼‰
        img_yuv = cv2.cvtColor(face_resized, cv2.COLOR_BGR2YUV)
        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])
        face_eq = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)

        # è½»å¾®é«˜æ–¯æ¨¡ç³Šå»å™ª
        face_blur = cv2.GaussianBlur(face_eq, (3, 3), 0)

        return face_blur

    def recognize_single_frame(self, face_img):
        """è¯†åˆ«å•å¸§ä¸­çš„äººè„¸"""
        if face_img is None:
            return None, 0.0, None

        # é¢„å¤„ç†
        processed_face = self.preprocess_face(face_img)
        if processed_face is None:
            return None, 0.0, None

        # å½’ä¸€åŒ–
        face_normalized = processed_face.astype(np.float32) / 255.0

        try:
            # è¿è¡Œæ¨ç†
            logits = self.sess.run(self.outdata,
                                   feed_dict={
                                       self.input_image: [face_normalized],
                                       self.dropout_rate: 1.0,  # æ¨ç†æ—¶dropoutç‡ä¸º1.0ï¼ˆä¸ä½¿ç”¨dropoutï¼‰
                                       self.dropout_rate_2: 1.0  # æ¨ç†æ—¶dropoutç‡ä¸º1.0ï¼ˆä¸ä½¿ç”¨dropoutï¼‰
                                   })

            # è®¡ç®—softmaxæ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / np.sum(exp_logits)
            probs = probs[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬

            # è·å–åŸå§‹é¢„æµ‹ç»“æœ
            raw_predicted = np.argmax(probs)
            raw_confidence = np.max(probs)

            return raw_predicted, raw_confidence, probs

        except Exception as e:
            print(f"æ¨ç†é”™è¯¯: {e}")
            return None, 0.0, None

    def recognize_with_smoothing(self, face_img):
        """ä½¿ç”¨æ—¶é—´å¹³æ»‘çš„è¯†åˆ«äººè„¸"""
        # è·å–å½“å‰å¸§çš„è¯†åˆ«ç»“æœ
        raw_predicted, raw_confidence, probs = self.recognize_single_frame(face_img)

        if raw_predicted is None:
            return None, 0.0, None

        # ä¿å­˜åˆ°å†å²
        self.prediction_history.append(raw_predicted)
        self.confidence_history.append(raw_confidence)

        # åº”ç”¨æ—¶é—´å¹³æ»‘ï¼šä½¿ç”¨å†å²æŠ•ç¥¨
        final_predicted = raw_predicted
        final_confidence = raw_confidence

        if len(self.prediction_history) >= 5:
            # è®¡ç®—æœ€è¿‘5æ¬¡é¢„æµ‹çš„ä¼—æ•°
            recent_predictions = list(self.prediction_history)[-5:]
            pred_counter = Counter(recent_predictions)
            most_common_pred, most_common_count = pred_counter.most_common(1)[0]

            # å¦‚æœä¼—æ•°å‡ºç°æ¬¡æ•°è¶…è¿‡3æ¬¡ï¼Œä¸”ä¸å½“å‰é¢„æµ‹ä¸åŒ
            if most_common_count >= 3 and most_common_pred != raw_predicted:
                final_predicted = most_common_pred
                # è®¡ç®—è¯¥ç±»åˆ«åœ¨å†å²ä¸­çš„å¹³å‡ç½®ä¿¡åº¦
                confidences = [conf for pred, conf in
                               zip(list(self.prediction_history), list(self.confidence_history))
                               if pred == most_common_pred]
                final_confidence = np.mean(confidences) if confidences else raw_confidence

        # åº”ç”¨åŠ¨æ€é˜ˆå€¼
        class_threshold = self.class_thresholds.get(final_predicted, self.base_threshold)

        # å¦‚æœç½®ä¿¡åº¦ä½äºç±»åˆ«é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯é™Œç”Ÿäºº
        if final_confidence < class_threshold:
            # æ‰¾åˆ°é™Œç”Ÿäººå¯¹åº”çš„ç±»åˆ«ç´¢å¼•
            stranger_idx = None
            for i, name in enumerate(self.class_names):
                if name == "é™Œç”Ÿäºº":
                    stranger_idx = i
                    break

            if stranger_idx is not None:
                final_predicted = stranger_idx
                final_confidence = probs[stranger_idx] if probs is not None else 0.0

        return final_predicted, final_confidence, probs

    def close(self):
        """å…³é—­èµ„æº"""
        if self.sess:
            self.sess.close()


class FaceRecognitionApp:
    def __init__(self, root):
        self.root = root
        self.root.title("äººè„¸è¯†åˆ«ç³»ç»Ÿ - å®Œæ•´ä¿®å¤ç‰ˆ - æ¨¡å‹åŠ è½½ä¿®å¤ - è‡ªåŠ¨é‡æ–°è¯†åˆ« - ç«‹å³é‡å¯")
        self.root.geometry("1200x800")  # å¢å¤§çª—å£

        # åˆå§‹åŒ–å˜é‡
        self.cap = None
        self.is_running = False
        self.is_collecting = False
        self.collection_count = 0
        self.max_collection = 500  # é»˜è®¤é‡‡é›†500å¼ 
        self.current_user = "default_user"
        self.camera_index = 0  # æ‘„åƒå¤´ç´¢å¼•
        self.target_count = 0
        self.save_dir = ""
        self.frame_skip = 1  # æé«˜é‡‡é›†é€Ÿåº¦
        self.frame_counter = 0

        # åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        self.face_recognizer = ImprovedFaceRecognizer()

        # åˆå§‹åŒ–äººè„¸é‡‡é›†å™¨
        self.face_collector = FaceDataCollector()

        # åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼ˆä¸faces_my.pyä¸€è‡´ï¼‰
        try:
            self.detector = dlib.get_frontal_face_detector()
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"æ— æ³•åŠ è½½dlibäººè„¸æ£€æµ‹å™¨: {e}\nè¯·ç¡®ä¿å·²å®‰è£…dlibåº“")
            raise

        # æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„
        self.check_project_structure()

        # å°è¯•åŠ è½½æ¨¡å‹
        self.model_loaded = self.face_recognizer.load_model()

        # åˆ›å»ºç•Œé¢
        self.create_widgets()

        # ç«‹å³å¯åŠ¨æ‘„åƒå¤´
        self.start_default_camera()

    def check_project_structure(self):
        """æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„"""
        print("æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„...")
        dirs_to_check = ['faces_ok', 'faces_no', 'model_multi_class']
        for dir_name in dirs_to_check:
            exists = os.path.exists(dir_name)
            print(f"ç›®å½• '{dir_name}': {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")

    def start_default_camera(self):
        """å¯åŠ¨é»˜è®¤æ‘„åƒå¤´"""
        if self.cap is None:
            self.cap = cv2.VideoCapture(self.camera_index)

        if not self.cap.isOpened():
            messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        self.is_running = True
        self.update_info("æ‘„åƒå¤´å·²å¯åŠ¨...")
        self.update_video()

    def create_widgets(self):
        # ä¸»æ¡†æ¶ - åˆ†å·¦å³ä¸¤åˆ—
        main_frame = ttk.Frame(self.root)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ (å 60%å®½åº¦)
        left_frame = ttk.Frame(main_frame, width=720)  # 1200 * 0.6 = 720
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        left_frame.pack_propagate(False)  # å›ºå®šå®½åº¦

        # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        video_frame = ttk.LabelFrame(left_frame, text="å®æ—¶ç”»é¢", padding=5)
        video_frame.pack(fill=tk.BOTH, expand=True)

        self.video_label = ttk.Label(video_frame, background="black", text="æ‘„åƒå¤´å·²å¯åŠ¨")
        self.video_label.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        # å³ä¾§ï¼šæ§åˆ¶é¢æ¿å’Œä¿¡æ¯åŒºåŸŸ (å 40%å®½åº¦)
        right_frame = ttk.Frame(main_frame, width=480)  # 1200 * 0.4 = 480
        right_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(5, 0))
        right_frame.pack_propagate(False)  # å›ºå®šå®½åº¦

        # é¡¹ç›®ä¿¡æ¯å’Œæ¨¡å‹çŠ¶æ€
        info_frame = ttk.LabelFrame(right_frame, text="é¡¹ç›®ä¿¡æ¯", padding=5)
        info_frame.pack(fill=tk.X, pady=(0, 10))

        # æ£€æŸ¥å¹¶æ˜¾ç¤ºé¡¹ç›®çŠ¶æ€
        status_text = "é¡¹ç›®çŠ¶æ€:\n"
        status_text += f"- faces_ok: {'âœ“' if os.path.exists('./faces_ok') else 'âœ—'}\n"
        status_text += f"- faces_no: {'âœ“' if os.path.exists('./faces_no') else 'âœ—'}\n"
        status_text += f"- model_multi_class: {'âœ“' if os.path.exists('./model_multi_class') else 'âœ—'}\n"

        ttk.Label(info_frame, text=status_text, justify=tk.LEFT, font=("Arial", 9)).pack(anchor=tk.W)

        # æ¨¡å‹çŠ¶æ€
        model_status_frame = ttk.LabelFrame(right_frame, text="æ¨¡å‹çŠ¶æ€", padding=5)
        model_status_frame.pack(fill=tk.X, pady=(0, 10))

        if self.model_loaded:
            status_text = f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!\n- ç±»åˆ«æ•°: {self.face_recognizer.num_classes}\n- ç±»åˆ«åç§°: {self.face_recognizer.class_names}"
            ttk.Label(model_status_frame, text=status_text, foreground="green", font=("Arial", 9)).pack(anchor=tk.W)
        else:
            status_text = "âœ— æœªæ£€æµ‹åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹\nğŸ’¡ è¯·å…ˆè¿è¡Œ faces_train_multi_person.py è®­ç»ƒæ¨¡å‹"
            ttk.Label(model_status_frame, text=status_text, foreground="red", font=("Arial", 9)).pack(anchor=tk.W)

        # æ§åˆ¶é¢æ¿
        control_frame = ttk.LabelFrame(right_frame, text="æ§åˆ¶é¢æ¿", padding=5)
        control_frame.pack(fill=tk.X, pady=(0, 10))

        # ç”¨æˆ·åè¾“å…¥
        username_frame = ttk.Frame(control_frame)
        username_frame.pack(fill=tk.X, pady=(0, 5))
        ttk.Label(username_frame, text="ç”¨æˆ·å:", font=("Arial", 9)).pack(side=tk.LEFT)
        self.user_entry = ttk.Entry(username_frame, width=15, font=("Arial", 9))
        self.user_entry.insert(0, self.current_user)
        self.user_entry.pack(side=tk.RIGHT)

        # é‡‡é›†æ•°é‡è¾“å…¥
        count_frame = ttk.Frame(control_frame)
        count_frame.pack(fill=tk.X, pady=(0, 5))
        ttk.Label(count_frame, text="é‡‡é›†æ•°é‡:", font=("Arial", 9)).pack(side=tk.LEFT)
        self.count_entry = ttk.Entry(count_frame, width=15, font=("Arial", 9))
        self.count_entry.insert(0, str(self.max_collection))
        self.count_entry.pack(side=tk.RIGHT)

        # æ‘„åƒå¤´é€‰æ‹©
        camera_frame = ttk.Frame(control_frame)
        camera_frame.pack(fill=tk.X, pady=(0, 5))
        ttk.Label(camera_frame, text="æ‘„åƒå¤´:", font=("Arial", 9)).pack(side=tk.LEFT)
        self.camera_var = tk.StringVar(value="å†…ç½®")
        camera_combo = ttk.Combobox(camera_frame, textvariable=self.camera_var, state="readonly", width=12)
        camera_combo['values'] = ("å†…ç½®(0)", "å¤–æ¥(1)")
        camera_combo.pack(side=tk.RIGHT)

        # æŒ‰é’®
        btn_frame = ttk.Frame(control_frame)
        btn_frame.pack(fill=tk.X, pady=(0, 5))

        self.collect_btn = ttk.Button(btn_frame, text="äººè„¸é‡‡é›†", command=self.toggle_collection, width=10)
        self.collect_btn.pack(side=tk.LEFT, padx=(0, 5))

        self.recognize_btn = ttk.Button(btn_frame, text="äººè„¸è¯†åˆ«", command=self.toggle_recognition, width=10)
        self.recognize_btn.pack(side=tk.LEFT, padx=(0, 5))
        if not self.model_loaded:
            self.recognize_btn.config(state=tk.DISABLED)

        self.stop_btn = ttk.Button(btn_frame, text="åœæ­¢", command=self.stop_all, state=tk.NORMAL, width=10)
        self.stop_btn.pack(side=tk.RIGHT)

        self.status_label = ttk.Label(control_frame, text="çŠ¶æ€: æ‘„åƒå¤´å·²å¯åŠ¨", font=("Arial", 9))
        self.status_label.pack(pady=(5, 0))

        # è¿›åº¦æ¡
        self.progress = ttk.Progressbar(control_frame, mode='determinate', length=200)
        self.progress['maximum'] = self.max_collection
        self.progress.pack(pady=(5, 0), fill=tk.X)
        self.progress.pack_forget()  # é»˜è®¤éšè—

        # è®­ç»ƒè¿›åº¦æ¡
        self.train_progress = ttk.Progressbar(control_frame, mode='indeterminate', length=200)
        self.train_progress.pack(pady=(5, 0), fill=tk.X)
        self.train_progress.pack_forget()  # é»˜è®¤éšè—

        # æ—¥å¿—ä¿¡æ¯åŒºåŸŸ
        log_frame = ttk.LabelFrame(right_frame, text="æ—¥å¿—ä¿¡æ¯", padding=5)
        log_frame.pack(fill=tk.BOTH, expand=True)

        self.info_text = scrolledtext.ScrolledText(log_frame, height=10, font=("Arial", 9))
        self.info_text.pack(fill=tk.BOTH, expand=True)

    def toggle_collection(self):
        if not self.is_running:
            messagebox.showwarning("è­¦å‘Š", "æ‘„åƒå¤´æœªå¯åŠ¨")
            return

        if not self.is_collecting:
            self.start_collection()
        else:
            self.stop_collection()

    def toggle_recognition(self):
        if not self.is_running:
            messagebox.showwarning("è­¦å‘Š", "æ‘„åƒå¤´æœªå¯åŠ¨")
            return

        if self.is_collecting:
            self.stop_collection()

        # åˆ‡æ¢è¯†åˆ«çŠ¶æ€
        if self.is_running and not self.is_collecting and self.model_loaded:
            self.is_running = False
            self.recognize_btn.config(text="äººè„¸è¯†åˆ«")
            self.status_label.config(text="çŠ¶æ€: æ‘„åƒå¤´å·²åœæ­¢è¯†åˆ«")
            self.update_info("äººè„¸è¯†åˆ«å·²åœæ­¢")
        else:
            if not self.model_loaded:
                messagebox.showwarning("è­¦å‘Š", "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
                return
            self.is_running = True
            self.recognize_btn.config(text="åœæ­¢è¯†åˆ«")
            self.status_label.config(text="çŠ¶æ€: æ­£åœ¨è¯†åˆ«äººè„¸")
            self.update_info("å¼€å§‹äººè„¸è¯†åˆ«...")
            self.update_video()

    def start_collection(self):
        self.current_user = self.user_entry.get().strip()
        if not self.current_user:
            self.current_user = "default_user"

        # è·å–é‡‡é›†æ•°é‡
        try:
            count = int(self.count_entry.get())
            if count < 20:
                count = 20
            self.max_collection = count
        except:
            self.max_collection = 500  # é»˜è®¤500å¼ 

        # è·å–æ‘„åƒå¤´ç´¢å¼•
        if self.camera_var.get() == "å¤–æ¥(1)":
            self.camera_index = 1
        else:
            self.camera_index = 0

        # é‡æ–°æ‰“å¼€æ‘„åƒå¤´ï¼ˆå¦‚æœéœ€è¦åˆ‡æ¢æ‘„åƒå¤´ï¼‰
        if self.cap is not None:
            self.cap.release()
        self.cap = cv2.VideoCapture(self.camera_index)
        if not self.cap.isOpened():
            messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        # åˆå§‹åŒ–é‡‡é›†å‚æ•°
        try:
            self.target_count, self.save_dir, self.frame_skip, self.frame_counter = self.face_collector.capture_data(
                self.current_user, self.max_collection)
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"åˆå§‹åŒ–é‡‡é›†å¤±è´¥: {e}")
            return

        self.is_collecting = True
        self.collection_count = 0
        # æ£€æŸ¥å·²æœ‰å›¾ç‰‡æ•°é‡
        existing_files = [f for f in os.listdir(self.save_dir) if f.endswith(('.jpg', '.png'))]
        self.collection_count = len(existing_files)

        self.collect_btn.config(text="åœæ­¢é‡‡é›†")
        self.recognize_btn.config(state=tk.DISABLED)
        self.progress.pack(pady=(5, 0), fill=tk.X)  # æ˜¾ç¤ºè¿›åº¦æ¡
        self.progress['maximum'] = self.target_count
        self.progress['value'] = self.collection_count
        self.status_label.config(text=f"çŠ¶æ€: æ­£åœ¨é‡‡é›†äººè„¸ ({self.collection_count}/{self.target_count})")
        self.update_info(f"å¼€å§‹é‡‡é›†ç”¨æˆ· '{self.current_user}' çš„äººè„¸æ•°æ®...")

    def stop_collection(self):
        self.is_collecting = False
        self.collect_btn.config(text="äººè„¸é‡‡é›†")
        self.recognize_btn.config(state=tk.NORMAL if self.model_loaded else tk.DISABLED)
        self.progress.pack_forget()  # éšè—è¿›åº¦æ¡
        self.status_label.config(text="çŠ¶æ€: æ‘„åƒå¤´å·²åœæ­¢é‡‡é›†")
        self.update_info(f"äººè„¸é‡‡é›†åœæ­¢ï¼Œå…±ä¿å­˜ {self.collection_count} å¼ å›¾ç‰‡")

    def start_training(self):
        """å¼€å§‹æ¨¡å‹è®­ç»ƒ"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        faces_ok_dir = './faces_ok'
        if not os.path.exists(faces_ok_dir):
            messagebox.showerror("é”™è¯¯", "faces_okç›®å½•ä¸å­˜åœ¨")
            return

        class_names = [d for d in os.listdir(faces_ok_dir) if os.path.isdir(os.path.join(faces_ok_dir, d))]
        if len(class_names) < 2:
            messagebox.showerror("é”™è¯¯", "è‡³å°‘éœ€è¦2ä¸ªç±»åˆ«æ‰èƒ½è®­ç»ƒæ¨¡å‹")
            return

        # æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰é‡‡é›†æˆ–è¯†åˆ«åœ¨è¿è¡Œ
        if self.is_collecting:
            self.stop_collection()
        if self.is_running:
            self.is_running = False
            self.recognize_btn.config(text="äººè„¸è¯†åˆ«")

        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
        self.train_progress.pack(pady=(5, 0), fill=tk.X)
        self.train_progress.start(10)
        self.status_label.config(text="çŠ¶æ€: æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
        self.update_info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # ç¦ç”¨ç›¸å…³æŒ‰é’®
        self.collect_btn.config(state=tk.DISABLED)
        self.recognize_btn.config(state=tk.DISABLED)

        # åœ¨æ–°çº¿ç¨‹ä¸­è®­ç»ƒæ¨¡å‹
        threading.Thread(target=self._train_model_thread, daemon=True).start()

    def _train_model_thread(self):
        """è®­ç»ƒæ¨¡å‹çš„çº¿ç¨‹å‡½æ•°"""
        try:
            success = train_model()
            self.root.after(0, self._training_complete, success)
        except Exception as e:
            print(f"è®­ç»ƒçº¿ç¨‹å‡ºé”™: {e}")
            self.root.after(0, self._training_complete, False)

    def _training_complete(self, success):
        """è®­ç»ƒå®Œæˆåçš„å›è°ƒ"""
        # åœæ­¢å¹¶éšè—è®­ç»ƒè¿›åº¦æ¡
        self.train_progress.stop()
        self.train_progress.pack_forget()

        # é‡æ–°å¯ç”¨æŒ‰é’®
        self.collect_btn.config(state=tk.NORMAL)

        if success:
            self.update_info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
            self.status_label.config(text="çŠ¶æ€: æ¨¡å‹è®­ç»ƒå®Œæˆ")

            # å…³é”®ä¿®å¤ï¼šå…ˆå…³é—­æ—§çš„è¯†åˆ«å™¨ä¼šè¯ï¼Œé‡ç½®TensorFlowå›¾
            if self.face_recognizer:
                try:
                    self.face_recognizer.close()
                    self.face_recognizer.sess = None
                except:
                    pass  # å¦‚æœå…³é—­å¤±è´¥ï¼Œå¿½ç•¥

            # é‡ç½®TensorFlowçš„é»˜è®¤å›¾
            tf.reset_default_graph()

            # åˆ›å»ºæ–°çš„è¯†åˆ«å™¨å®ä¾‹
            self.face_recognizer = ImprovedFaceRecognizer()

            # å°è¯•å¤šæ¬¡åŠ è½½æ¨¡å‹ï¼Œæ¯æ¬¡ä¹‹é—´æœ‰å»¶è¿Ÿ
            max_retries = 3
            for retry in range(max_retries):
                self.model_loaded = self.face_recognizer.load_model()
                if self.model_loaded:
                    break
                if retry < max_retries - 1:
                    self.update_info(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retry + 1}/{max_retries})...")
                    time.sleep(1)  # ç­‰å¾…1ç§’å†é‡è¯•

            # æ›´æ–°æ¨¡å‹çŠ¶æ€æ˜¾ç¤º
            model_status_frame = self.root.nametowidget(
                self.root.winfo_children()[0].winfo_children()[1].winfo_children()[1])
            for widget in model_status_frame.winfo_children():
                widget.destroy()

            if self.model_loaded:
                status_text = f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!\n- ç±»åˆ«æ•°: {self.face_recognizer.num_classes}\n- ç±»åˆ«åç§°: {self.face_recognizer.class_names}"
                ttk.Label(model_status_frame, text=status_text, foreground="green", font=("Arial", 9)).pack(anchor=tk.W)
                self.recognize_btn.config(state=tk.NORMAL)

                # ç«‹å³å¼€å§‹äººè„¸è¯†åˆ«
                self.is_running = True
                self.recognize_btn.config(text="åœæ­¢è¯†åˆ«")
                self.status_label.config(text="çŠ¶æ€: æ­£åœ¨è¯†åˆ«äººè„¸")
                self.update_info("è®­ç»ƒå®Œæˆï¼Œç«‹å³å¼€å§‹äººè„¸è¯†åˆ«...")

                # ç«‹å³å¼€å§‹æ›´æ–°è§†é¢‘ï¼ˆè§¦å‘è¯†åˆ«ï¼‰
                self.update_video()
            else:
                status_text = "âœ— æ¨¡å‹åŠ è½½å¤±è´¥\nğŸ’¡ è¯·å°è¯•é‡æ–°å¯åŠ¨ç¨‹åº"
                ttk.Label(model_status_frame, text=status_text, foreground="red", font=("Arial", 9)).pack(anchor=tk.W)
                self.recognize_btn.config(state=tk.DISABLED)
                self.update_info("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·é‡æ–°å¯åŠ¨ç¨‹åº")
        else:
            self.update_info("æ¨¡å‹è®­ç»ƒå¤±è´¥")
            self.status_label.config(text="çŠ¶æ€: æ¨¡å‹è®­ç»ƒå¤±è´¥")
            messagebox.showerror("é”™è¯¯", "æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥faces_okç›®å½•ä¸­çš„æ•°æ®")

            # é‡æ–°å¯ç”¨è¯†åˆ«æŒ‰é’®
            self.recognize_btn.config(state=tk.NORMAL if self.model_loaded else tk.DISABLED)

    def stop_all(self):
        """åœæ­¢æ‰€æœ‰æ“ä½œä½†ä¿æŒæ‘„åƒå¤´è¿è¡Œ"""
        self.is_running = False
        self.is_collecting = False
        self.recognize_btn.config(text="äººè„¸è¯†åˆ«", state=tk.NORMAL if self.model_loaded else tk.DISABLED)
        self.collect_btn.config(text="äººè„¸é‡‡é›†", state=tk.NORMAL)
        self.progress.pack_forget()  # éšè—è¿›åº¦æ¡
        self.train_progress.stop()
        self.train_progress.pack_forget()  # éšè—è®­ç»ƒè¿›åº¦æ¡
        self.status_label.config(text="çŠ¶æ€: æ‘„åƒå¤´å·²åœæ­¢")
        self.update_info("æ‘„åƒå¤´å·²åœæ­¢")

    def update_video(self):
        if not self.cap or not self.cap.isOpened():
            return

        ret, frame = self.cap.read()
        if not ret:
            self.root.after(10, self.update_video)
            return

        # æ£€æµ‹äººè„¸ï¼ˆä¸faces_my.pyä¸€è‡´ï¼‰
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.detector(gray, 1)

        for i, d in enumerate(faces):
            x1 = max(d.top(), 0)
            y1 = min(d.bottom(), frame.shape[0])
            x2 = max(d.left(), 0)
            y2 = min(d.right(), frame.shape[1])

            if self.is_collecting:
                # é‡‡é›†æ¨¡å¼ï¼šæ£€æµ‹åˆ°äººè„¸æ—¶ä¿å­˜ç…§ç‰‡
                face_img = frame[x1:y1, x2:y2]
                if face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:  # ç¡®ä¿äººè„¸è¶³å¤Ÿå¤§
                    self.frame_counter += 1

                    # æ¯frame_skipå¸§é‡‡é›†ä¸€æ¬¡
                    if self.frame_counter % self.frame_skip == 0 and self.collection_count < self.target_count:
                        face_resized = cv2.resize(face_img, (64, 64))

                        # ç”Ÿæˆæ—¶é—´æˆ³ä½œä¸ºæ–‡ä»¶å
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
                        base_filename = f"{self.current_user}_{timestamp}"

                        # ä¿å­˜åŸå§‹å›¾ç‰‡
                        original_path = os.path.join(self.save_dir, f"{base_filename}_orig.jpg")
                        cv2.imwrite(original_path, face_resized)
                        self.collection_count += 1

                        # è‡ªåŠ¨ç”Ÿæˆ3ä¸ªå¢å¼ºç‰ˆæœ¬
                        if self.collection_count < self.target_count:
                            augmentations = self.face_collector.apply_augmentations(face_resized)
                            for i, aug_img in enumerate(augmentations):
                                if self.collection_count >= self.target_count:
                                    break

                                aug_path = os.path.join(self.save_dir, f"{base_filename}_aug{i + 1}.jpg")
                                cv2.imwrite(aug_path, aug_img)
                                self.collection_count += 1

                        # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€
                        self.progress['value'] = self.collection_count
                        self.status_label.config(
                            text=f"çŠ¶æ€: æ­£åœ¨é‡‡é›†äººè„¸ ({self.collection_count}/{self.target_count})")

                        if self.collection_count >= self.target_count:
                            self.stop_collection()
                            # è‡ªåŠ¨å¼€å§‹è®­ç»ƒæ¨¡å‹
                            self.start_training()
                            break

                # ç»˜åˆ¶ç»¿è‰²é‡‡é›†æ¡†
                cv2.rectangle(frame, (x2, x1), (y2, y1), (0, 255, 0), 2)  # ç»¿è‰²æ¡†

                # ä½¿ç”¨OpenCVç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ï¼ˆè§£å†³ä¹±ç é—®é¢˜ï¼‰
                # å…ˆåˆ›å»ºä¸€ä¸ªPILå›¾åƒï¼Œç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ï¼Œç„¶åè½¬å›OpenCVæ ¼å¼
                img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(img_pil)
                try:
                    # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                    font = ImageFont.truetype("simhei.ttf", 20)
                except:
                    try:
                        font = ImageFont.truetype("Arial.ttf", 20)
                    except:
                        font = ImageFont.load_default()

                draw.text((x2, y1 - 25), f"é‡‡é›†ä¸­: {self.collection_count}/{self.target_count}", font=font,
                          fill=(0, 255, 0))
                frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
            elif self.is_running and not self.is_collecting and self.model_loaded:
                # è¯†åˆ«æ¨¡å¼ï¼šè¯†åˆ«äººè„¸
                face_img = frame[x1:y1, x2:y2]
                if face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:  # ç¡®ä¿äººè„¸è¶³å¤Ÿå¤§
                    # è¯†åˆ«äººè„¸ï¼ˆä½¿ç”¨face_recognition_multi_person.pyçš„æ”¹è¿›è¯†åˆ«ï¼‰
                    predicted_class_idx, confidence, all_probs = self.face_recognizer.recognize_with_smoothing(face_img)

                    # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè¯†åˆ«ç»“æœä¸ºNoneï¼Œè·³è¿‡
                    if predicted_class_idx is not None:
                        # è·å–ç±»åˆ«åç§°
                        if predicted_class_idx < len(self.face_recognizer.class_names):
                            person_name = self.face_recognizer.class_names[predicted_class_idx]
                        else:
                            person_name = "æœªçŸ¥"

                        # æ ¹æ®ç½®ä¿¡åº¦è®¾ç½®æ ‡ç­¾
                        if confidence > 0.55:
                            if person_name == "é™Œç”Ÿäºº":
                                color = (0, 0, 255)  # çº¢è‰² - é™Œç”Ÿäºº
                                label = f"é™Œç”Ÿäºº ({confidence:.2f})"
                            else:
                                color = (0, 255, 0)  # ç»¿è‰² - å·²çŸ¥äººå‘˜
                                label = f"{person_name} ({confidence:.2f})"
                        else:
                            color = (128, 128, 128)  # ç°è‰² - ç½®ä¿¡åº¦ä½
                            label = "ä½ç½®ä¿¡åº¦"

                        # ä½¿ç”¨OpenCVç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ï¼ˆè§£å†³ä¹±ç é—®é¢˜ï¼‰
                        # å…ˆåˆ›å»ºä¸€ä¸ªPILå›¾åƒï¼Œç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ï¼Œç„¶åè½¬å›OpenCVæ ¼å¼
                        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        from PIL import ImageDraw, ImageFont
                        draw = ImageDraw.Draw(img_pil)
                        try:
                            # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                            font = ImageFont.truetype("simhei.ttf", 20)
                        except:
                            try:
                                font = ImageFont.truetype("Arial.ttf", 20)
                            except:
                                font = ImageFont.load_default()

                        draw.text((x2, y1 - 25), label, font=font, fill=color)
                        frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

                        # æ›´æ–°ä¿¡æ¯æ˜¾ç¤º
                        info_msg = f"è¯†åˆ«ç»“æœ: {person_name}, ç½®ä¿¡åº¦: {confidence:.3f}"
                        self.update_info(info_msg)
                    else:
                        # å¦‚æœè¯†åˆ«å¤±è´¥ï¼Œæ˜¾ç¤º"è¯†åˆ«å¤±è´¥"
                        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        from PIL import ImageDraw, ImageFont
                        draw = ImageDraw.Draw(img_pil)
                        try:
                            # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                            font = ImageFont.truetype("simhei.ttf", 20)
                        except:
                            try:
                                font = ImageFont.truetype("Arial.ttf", 20)
                            except:
                                font = ImageFont.load_default()

                        draw.text((x2, y1 - 25), "è¯†åˆ«å¤±è´¥", font=font, fill=(128, 128, 128))
                        frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

                # ç»˜åˆ¶è“è‰²è¯†åˆ«æ¡†
                cv2.rectangle(frame, (x2, x1), (y2, y1), (255, 0, 0), 2)  # è“è‰²æ¡†
            else:
                # æ™®é€šæ˜¾ç¤ºæ¨¡å¼ï¼ˆæ‘„åƒå¤´å¼€å¯ä½†ä¸è¿›è¡Œè¯†åˆ«æˆ–é‡‡é›†ï¼‰
                cv2.rectangle(frame, (x2, x1), (y2, y1), (255, 255, 0), 2)  # é»„è‰²æ¡†

        # è½¬æ¢ä¸ºPILå›¾åƒå¹¶æ˜¾ç¤º
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(frame_rgb)
        img_pil = img_pil.resize((700, 500), Image.Resampling.LANCZOS)
        img_tk = ImageTk.PhotoImage(img_pil)

        self.video_label.img_tk = img_tk  # ä¿æŒå¼•ç”¨
        self.video_label.configure(image=img_tk)

        # æ¯10æ¯«ç§’æ›´æ–°ä¸€æ¬¡
        self.root.after(10, self.update_video)

    def update_info(self, message):
        """æ›´æ–°ä¿¡æ¯æ˜¾ç¤ºåŒºåŸŸ"""
        current_time = time.strftime("%H:%M:%S", time.localtime())
        self.info_text.insert(tk.END, f"[{current_time}] {message}\n")
        self.info_text.see(tk.END)

    def close_app(self):
        # å…³é—­æ‘„åƒå¤´
        if self.cap is not None:
            self.cap.release()
            self.cap = None
        # å…³é—­è¯†åˆ«å™¨
        if hasattr(self, 'face_recognizer'):
            self.face_recognizer.close()
        # å…³é—­é‡‡é›†å™¨
        if hasattr(self, 'face_collector'):
            # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„é‡‡é›†ï¼Œéœ€è¦å¤„ç†
            pass
        self.root.destroy()


def main():
    root = tk.Tk()
    app = FaceRecognitionApp(root)
    root.protocol("WM_DELETE_WINDOW", app.close_app)
    root.mainloop()


if __name__ == "__main__":
    main()



