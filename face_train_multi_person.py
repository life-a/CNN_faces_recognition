import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import cv2
import dlib
import numpy as np
import tensorflow.compat.v1 as tf
import os
import time
from PIL import Image, ImageTk
import glob
from collections import deque, Counter
import random
from datetime import datetime


def layer_net(input_image, num_class, dropout_rate, dropout_rate_2):
    """å®Œå…¨æŒ‰ç…§åŸä»£ç å®šä¹‰"""
    tf.disable_eager_execution()

    """ç¬¬ä¸€ã€äºŒå±‚ï¼Œè¾“å…¥å›¾ç‰‡64*64*3ï¼Œè¾“å‡ºå›¾ç‰‡32*32*32"""
    w1 = tf.Variable(tf.random.normal([3, 3, 3, 32]), name='w1')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(3)ï¼Œ è¾“å‡ºé€šé“(32)
    b1 = tf.Variable(tf.random.normal([32]), name='b1')
    layer_conv1 = tf.nn.relu(
        tf.nn.conv2d(input_image, w1, strides=[1, 1, 1, 1], padding='SAME') + b1)  # 64*64*32ï¼Œå·ç§¯æå–ç‰¹å¾ï¼Œå¢åŠ é€šé“æ•°
    layer_pool1 = tf.nn.max_pool2d(layer_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                                      padding='SAME')  # 32*32*32ï¼Œæ± åŒ–é™ç»´ï¼Œå‡å°å¤æ‚åº¦
    drop1 = tf.nn.dropout(layer_pool1, rate=1 - dropout_rate)  # æŒ‰ä¸€å®šæ¦‚ç‡éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼Œä»¥è·å¾—æ›´é«˜çš„è®­ç»ƒé€Ÿåº¦ä»¥åŠé˜²æ­¢è¿‡æ‹Ÿåˆ

    """ç¬¬ä¸‰ã€å››å±‚ï¼Œè¾“å…¥å›¾ç‰‡32*32*32ï¼Œè¾“å‡ºå›¾ç‰‡16*16*64"""
    w2 = tf.Variable(tf.random.normal([3, 3, 32, 64]), name='w2')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(32)ï¼Œ è¾“å‡ºé€šé“(64)
    b2 = tf.Variable(tf.random.normal([64]), name='b2')
    layer_conv2 = tf.nn.relu(tf.nn.conv2d(drop1, w2, strides=[1, 1, 1, 1], padding='SAME') + b2)  # 32*32*64
    layer_pool2 = tf.nn.max_pool2d(layer_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 16*16*64
    drop2 = tf.nn.dropout(layer_pool2, rate=1 - dropout_rate)

    """ç¬¬äº”ã€å…­å±‚ï¼Œè¾“å…¥å›¾ç‰‡16*16*64ï¼Œè¾“å‡ºå›¾ç‰‡8*8*64"""
    w3 = tf.Variable(tf.random.normal([3, 3, 64, 64]), name='w3')  # å·ç§¯æ ¸å¤§å°(3,3)ï¼Œ è¾“å…¥é€šé“(64)ï¼Œ è¾“å‡ºé€šé“(64)
    b3 = tf.Variable(tf.random.normal([64]), name='b3')
    layer_conv3 = tf.nn.relu(tf.nn.conv2d(drop2, w3, strides=[1, 1, 1, 1], padding='SAME') + b3)  # 16*16*64
    layer_pool3 = tf.nn.max_pool2d(layer_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                                      padding='SAME')  # 8*8*64=4096
    drop3 = tf.nn.dropout(layer_pool3, rate=1 - dropout_rate)

    """ç¬¬ä¸ƒå±‚ï¼Œå…¨è¿æ¥å±‚ï¼Œå°†å›¾ç‰‡çš„å·ç§¯è¾“å‡ºå‹æ‰æˆä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œè¾“å…¥å›¾ç‰‡8*8*64ï¼Œreshapeåˆ°1*4096ï¼Œè¾“å‡º1*512"""
    w4 = tf.Variable(tf.random.normal([8 * 8 * 64, 512]), name='w4')  # è¾“å…¥é€šé“(4096)ï¼Œ è¾“å‡ºé€šé“(512)
    b4 = tf.Variable(tf.random.normal([512]), name='b4')
    layer_fully_connected = tf.reshape(drop3, [-1, 8 * 8 * 64])  # -1è¡¨ç¤ºè¡Œéšç€åˆ—çš„éœ€æ±‚æ”¹å˜ï¼Œ1*4096
    relu = tf.nn.relu(tf.matmul(layer_fully_connected, w4) + b4)  # [1,4096]*[4096,512]=[1,512]
    drop4 = tf.nn.dropout(relu, rate=1 - dropout_rate_2)

    """ç¬¬å…«å±‚ï¼Œè¾“å‡ºå±‚ï¼Œè¾“å…¥1*512ï¼Œè¾“å‡º1*2ï¼Œå†add"""
    w5 = tf.Variable(tf.random.normal([512, num_class]), name='w5')  # è¾“å…¥é€šé“(512)ï¼Œ è¾“å‡ºé€šé“(2)
    b5 = tf.Variable(tf.random.normal([num_class]), name='b5')
    outdata = tf.add(tf.matmul(drop4, w5), b5)  # (1,512)*(512,2)=(1,2) ,è·Ÿinput_label [0,1]ã€[1,0]æ¯”è¾ƒç»™å‡ºæŸå¤± ï¼Œå…ˆä¹˜å†åŠ 
    return outdata


class FaceDataCollector:
    """äººè„¸æ•°æ®é‡‡é›†å™¨ï¼ˆæ ¸å¿ƒåŠŸèƒ½ç‰ˆï¼‰"""

    def __init__(self, size=64):
        self.size = size
        self.detector = dlib.get_frontal_face_detector()

    def apply_augmentations(self, img):
        """åº”ç”¨ä¸‰ç§æ•°æ®å¢å¼º"""
        augmentations = []

        # 1. æ°´å¹³ç¿»è½¬
        flipped = cv2.flip(img, 1)
        augmentations.append(flipped)

        # 2. äº®åº¦è°ƒæ•´ï¼ˆéšæœºå˜äº®æˆ–å˜æš—ï¼‰
        alpha = random.uniform(0.7, 1.3)  # å¯¹æ¯”åº¦
        beta = random.randint(-30, 30)    # äº®åº¦
        bright = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)
        augmentations.append(bright)

        # 3. å¯¹æ¯”åº¦è°ƒæ•´
        contrast = cv2.convertScaleAbs(img, alpha=random.uniform(0.8, 1.2), beta=0)
        augmentations.append(contrast)

        return augmentations

    def capture_data(self, person_name, target_count=50, cap=None):
        """
        é‡‡é›†æŒ‡å®šäººå‘˜çš„äººè„¸æ•°æ®
        :param person_name: äººå‘˜åç§°
        :param target_count: ç›®æ ‡é‡‡é›†æ•°é‡
        :param cap: æ‘„åƒå¤´å¯¹è±¡
        """
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼šfaces_ok/äººå‘˜åç§°/
        save_dir = os.path.join('./faces_ok', person_name)
        os.makedirs(save_dir, exist_ok=True)

        # æ£€æŸ¥å·²æœ‰å›¾ç‰‡æ•°é‡
        existing_files = [f for f in os.listdir(save_dir) if f.endswith(('.jpg', '.png'))]
        saved_count = len(existing_files)

        # æ‰“å¼€æ‘„åƒå¤´ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if cap is None:
            cap = cv2.VideoCapture(0)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return False

        frame_skip = 3  # æ¯3å¸§é‡‡é›†ä¸€æ¬¡ï¼Œé¿å…è¿‡äºç›¸ä¼¼
        frame_counter = 0

        while saved_count < target_count:
            ret, frame = cap.read()
            if not ret:
                break

            frame_counter += 1

            # äººè„¸æ£€æµ‹
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.detector(gray, 1)

            # å¤„ç†æ£€æµ‹åˆ°çš„äººè„¸
            for face in faces:
                x1 = max(face.top(), 0)
                y1 = min(face.bottom(), frame.shape[0])
                x2 = max(face.left(), 0)
                y2 = min(face.right(), frame.shape[1])

                face_img = frame[x1:y1, x2:y2]
                if face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:
                    # è°ƒæ•´åˆ°æ ‡å‡†å°ºå¯¸
                    face_resized = cv2.resize(face_img, (self.size, self.size))

                    # ç”Ÿæˆæ—¶é—´æˆ³ä½œä¸ºæ–‡ä»¶å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
                    base_filename = f"{person_name}_{timestamp}"

                    # ä¿å­˜åŸå§‹å›¾ç‰‡
                    original_path = os.path.join(save_dir, f"{base_filename}_orig.jpg")
                    cv2.imwrite(original_path, face_resized)
                    saved_count += 1

                    # è‡ªåŠ¨ç”Ÿæˆ3ä¸ªå¢å¼ºç‰ˆæœ¬
                    if saved_count < target_count:
                        augmentations = self.apply_augmentations(face_resized)
                        for i, aug_img in enumerate(augmentations):
                            if saved_count >= target_count:
                                break

                            aug_path = os.path.join(save_dir, f"{base_filename}_aug{i+1}.jpg")
                            cv2.imwrite(aug_path, aug_img)
                            saved_count += 1

                    # æ›´æ–°è¿›åº¦
                    progress = min(saved_count / target_count, 1.0)
                    print(f"âœ… å·²ä¿å­˜ {saved_count}/{target_count} (è¿›åº¦: {progress*100:.1f}%)")
                    break  # åªå¤„ç†ç¬¬ä¸€å¼ è„¸

            if frame_counter % 10 == 0:  # æ¯10å¸§æ£€æŸ¥ä¸€æ¬¡
                break  # åœ¨GUIä¸­ï¼Œæˆ‘ä»¬åªé‡‡é›†ä¸€å¸§ç„¶åè¿”å›

        return True


class ImprovedFaceRecognizer:
    """æ”¹è¿›çš„äººè„¸è¯†åˆ«å™¨ï¼Œè§£å†³æ€»æ˜¯è¯†åˆ«ä¸ºåŒä¸€ä¸ªäººçš„é—®é¢˜"""

    def __init__(self, model_path='./model_multi_class/'):
        self.model_path = model_path
        self.sess = None
        self.outdata = None
        self.input_image = None
        self.dropout_rate = None
        self.dropout_rate_2 = None
        self.class_names = []
        self.num_classes = 0

        # æ—¶é—´å¹³æ»‘å‚æ•°
        self.prediction_history = deque(maxlen=15)  # ä¿å­˜æœ€è¿‘15æ¬¡é¢„æµ‹
        self.confidence_history = deque(maxlen=15)   # ä¿å­˜æœ€è¿‘15æ¬¡ç½®ä¿¡åº¦

        # åŠ¨æ€é˜ˆå€¼å‚æ•°
        self.base_threshold = 0.65  # åŸºç¡€ç½®ä¿¡åº¦é˜ˆå€¼
        self.class_thresholds = {}   # æ¯ä¸ªç±»åˆ«çš„åŠ¨æ€é˜ˆå€¼

    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„å¹³è¡¡æ¨¡å‹"""
        print(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æ¨¡å‹...")

        # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.model_path):
            print(f"é”™è¯¯: æ¨¡å‹ç›®å½• {self.model_path} ä¸å­˜åœ¨")
            return False

        # å°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹checkpoint
        checkpoint_path = self.find_latest_model()
        if checkpoint_path is None:
            print("æœªæ‰¾åˆ°æ¨¡å‹checkpoint")
            return False

        # è¯»å–ç±»åˆ«åç§°
        class_names_file = './model_multi_class/class_names.txt'
        if os.path.exists(class_names_file):
            with open(class_names_file, 'r', encoding='utf-8') as f:
                self.class_names = [line.strip() for line in f.readlines() if line.strip()]
        else:
            # å°è¯•ä»faces_okç›®å½•æ¨æ–­
            faces_ok_dir = './faces_ok'
            if os.path.exists(faces_ok_dir):
                class_names = []
                for item in os.listdir(faces_ok_dir):
                    item_path = os.path.join(faces_ok_dir, item)
                    if os.path.isdir(item_path):
                        class_names.append(item)
                # æ·»åŠ é™Œç”Ÿäººç±»åˆ«
                class_names.append("é™Œç”Ÿäºº")
                self.class_names = class_names
            else:
                print("faces_okç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«")
                self.class_names = ["æˆ‘çš„äººè„¸", "å…¶ä»–äººè„¸"]

        self.num_classes = len(self.class_names)
        print(f"åŠ è½½äº† {self.num_classes} ä¸ªç±»åˆ«: {self.class_names}")

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆå§‹åŒ–åŠ¨æ€é˜ˆå€¼
        for i, name in enumerate(self.class_names):
            if name == "é™Œç”Ÿäºº":
                self.class_thresholds[i] = 0.55  # é™Œç”Ÿäººé˜ˆå€¼è¾ƒä½
            else:
                self.class_thresholds[i] = 0.65  # å·²çŸ¥äººå‘˜é˜ˆå€¼è¾ƒé«˜

        # å®šä¹‰å ä½ç¬¦
        size = 64
        self.input_image = tf.placeholder(tf.float32, [None, size, size, 3], name='input_image')
        self.dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
        self.dropout_rate_2 = tf.placeholder(tf.float32, name='dropout_rate_2')

        # æ„å»ºç½‘ç»œ
        self.outdata = layer_net(self.input_image, self.num_classes,
                                 self.dropout_rate, self.dropout_rate_2)

        # åˆ›å»ºä¼šè¯
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)

        saver = tf.train.Saver()

        try:
            saver.restore(self.sess, checkpoint_path)
            print("æ¨¡å‹åŠ è½½æˆåŠŸ")

            # æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_input = np.random.randn(1, 64, 64, 3) * 0.1
            probs = self.sess.run(tf.nn.softmax(self.outdata),
                                 feed_dict={
                                     self.input_image: test_input,
                                     self.dropout_rate: 1.0,
                                     self.dropout_rate_2: 1.0
                                 })
            print(f"æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºshape: {probs.shape}")
            return True

        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def find_latest_model(self):
        """æŸ¥æ‰¾æœ€æ–°çš„best_modelæ–‡ä»¶"""
        model_dir = './model_multi_class/'
        if not os.path.exists(model_dir):
            print(f"æ¨¡å‹ç›®å½• {model_dir} ä¸å­˜åœ¨")
            return None

        # é¦–å…ˆå°è¯•ä»checkpointæ–‡ä»¶è·å–æ¨¡å‹è·¯å¾„
        try:
            checkpoint_file = os.path.join(model_dir, 'checkpoint')
            if os.path.exists(checkpoint_file):
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    for line in lines:
                        if 'model_checkpoint_path' in line:
                            model_name = line.split('"')[1]  # æå–æ¨¡å‹åç§°
                            model_path = os.path.join(model_dir, model_name)
                            print(f"ä»checkpointæ–‡ä»¶æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {model_path}")
                            return model_path
        except Exception as e:
            print(f"è¯»å–checkpointæ–‡ä»¶å¤±è´¥: {e}")

        # å¦‚æœcheckpointæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨æŸ¥æ‰¾best_modelæ–‡ä»¶
        pattern = os.path.join(model_dir, 'best_model-*')
        files = glob.glob(pattern)

        if not files:
            print("æœªæ‰¾åˆ°ä»»ä½•best_modelæ–‡ä»¶")
            return None

        # æŒ‰æ­¥æ•°æ’åºï¼Œæ‰¾åˆ°æ­¥æ•°æœ€å¤§çš„
        model_steps = []
        for f in files:
            try:
                # æå–æ­¥æ•°ï¼ˆæ–‡ä»¶åæ ¼å¼: best_model-æ­¥æ•°.meta/data/indexï¼‰
                base_name = os.path.basename(f)
                if '-' in base_name:
                    step_str = base_name.split('-')[1].split('.')[0]
                    if step_str.isdigit():
                        step = int(step_str)
                        model_steps.append((step, f))
            except:
                continue

        if not model_steps:
            print("æ— æ³•ä»æ–‡ä»¶åä¸­æå–æ­¥æ•°")
            return None

        # è¿”å›æ­¥æ•°æœ€å¤§çš„æ¨¡å‹
        max_step, max_file = max(model_steps, key=lambda x: x[0])
        # è·å–åŸºæœ¬è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
        base_path = max_file.split('.')[0]
        print(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {base_path} (æ­¥æ•°: {max_step})")
        return base_path

    def preprocess_face(self, face_img):
        """é¢„å¤„ç†äººè„¸å›¾åƒ"""
        if face_img is None or face_img.size == 0:
            return None

        # è°ƒæ•´å¤§å°åˆ°64x64
        face_resized = cv2.resize(face_img, (64, 64))

        # ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆå¢å¼ºå¯¹æ¯”åº¦ï¼‰
        img_yuv = cv2.cvtColor(face_resized, cv2.COLOR_BGR2YUV)
        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
        face_eq = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)

        # è½»å¾®é«˜æ–¯æ¨¡ç³Šå»å™ª
        face_blur = cv2.GaussianBlur(face_eq, (3, 3), 0)

        return face_blur

    def recognize_single_frame(self, face_img):
        """è¯†åˆ«å•å¸§ä¸­çš„äººè„¸"""
        if face_img is None:
            return None, 0.0, None

        # é¢„å¤„ç†
        processed_face = self.preprocess_face(face_img)
        if processed_face is None:
            return None, 0.0, None

        # å½’ä¸€åŒ–
        face_normalized = processed_face.astype(np.float32) / 255.0

        try:
            # è¿è¡Œæ¨ç†
            logits = self.sess.run(self.outdata,
                                  feed_dict={
                                      self.input_image: [face_normalized],
                                      self.dropout_rate: 1.0,  # æ¨ç†æ—¶dropoutç‡ä¸º1.0ï¼ˆä¸ä½¿ç”¨dropoutï¼‰
                                      self.dropout_rate_2: 1.0  # æ¨ç†æ—¶dropoutç‡ä¸º1.0ï¼ˆä¸ä½¿ç”¨dropoutï¼‰
                                  })

            # è®¡ç®—softmaxæ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / np.sum(exp_logits)
            probs = probs[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬

            # è·å–åŸå§‹é¢„æµ‹ç»“æœ
            raw_predicted = np.argmax(probs)
            raw_confidence = np.max(probs)

            return raw_predicted, raw_confidence, probs

        except Exception as e:
            print(f"æ¨ç†é”™è¯¯: {e}")
            return None, 0.0, None

    def recognize_with_smoothing(self, face_img):
        """ä½¿ç”¨æ—¶é—´å¹³æ»‘çš„è¯†åˆ«äººè„¸"""
        # è·å–å½“å‰å¸§çš„è¯†åˆ«ç»“æœ
        raw_predicted, raw_confidence, probs = self.recognize_single_frame(face_img)

        if raw_predicted is None:
            return None, 0.0, None

        # ä¿å­˜åˆ°å†å²
        self.prediction_history.append(raw_predicted)
        self.confidence_history.append(raw_confidence)

        # åº”ç”¨æ—¶é—´å¹³æ»‘ï¼šä½¿ç”¨å†å²æŠ•ç¥¨
        final_predicted = raw_predicted
        final_confidence = raw_confidence

        if len(self.prediction_history) >= 5:
            # è®¡ç®—æœ€è¿‘5æ¬¡é¢„æµ‹çš„ä¼—æ•°
            recent_predictions = list(self.prediction_history)[-5:]
            pred_counter = Counter(recent_predictions)
            most_common_pred, most_common_count = pred_counter.most_common(1)[0]

            # å¦‚æœä¼—æ•°å‡ºç°æ¬¡æ•°è¶…è¿‡3æ¬¡ï¼Œä¸”ä¸å½“å‰é¢„æµ‹ä¸åŒ
            if most_common_count >= 3 and most_common_pred != raw_predicted:
                final_predicted = most_common_pred
                # è®¡ç®—è¯¥ç±»åˆ«åœ¨å†å²ä¸­çš„å¹³å‡ç½®ä¿¡åº¦
                confidences = [conf for pred, conf in
                              zip(list(self.prediction_history), list(self.confidence_history))
                              if pred == most_common_pred]
                final_confidence = np.mean(confidences) if confidences else raw_confidence

        # åº”ç”¨åŠ¨æ€é˜ˆå€¼
        class_threshold = self.class_thresholds.get(final_predicted, self.base_threshold)

        # å¦‚æœç½®ä¿¡åº¦ä½äºç±»åˆ«é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯é™Œç”Ÿäºº
        if final_confidence < class_threshold:
            # æ‰¾åˆ°é™Œç”Ÿäººå¯¹åº”çš„ç±»åˆ«ç´¢å¼•
            stranger_idx = None
            for i, name in enumerate(self.class_names):
                if name == "é™Œç”Ÿäºº":
                    stranger_idx = i
                    break

            if stranger_idx is not None:
                final_predicted = stranger_idx
                final_confidence = probs[stranger_idx] if probs is not None else 0.0

        return final_predicted, final_confidence, probs

    def get_class_color(self, class_idx, confidence):
        """æ ¹æ®ç±»åˆ«å’Œç½®ä¿¡åº¦è·å–æ˜¾ç¤ºé¢œè‰²"""
        if class_idx >= len(self.class_names):
            return (128, 128, 128)  # ç°è‰² - æœªçŸ¥ç±»åˆ«

        class_name = self.class_names[class_idx]

        if class_name == "é™Œç”Ÿäºº":
            if confidence > 0.7:
                return (0, 0, 255)  # çº¢è‰² - é«˜ç½®ä¿¡åº¦é™Œç”Ÿäºº
            else:
                return (0, 165, 255)  # æ©™è‰² - ä½ç½®ä¿¡åº¦é™Œç”Ÿäºº
        else:
            if confidence > 0.75:
                return (0, 255, 0)  # ç»¿è‰² - é«˜ç½®ä¿¡åº¦å·²çŸ¥äººå‘˜
            elif confidence > 0.6:
                return (255, 255, 0)  # é»„è‰² - ä¸­ç­‰ç½®ä¿¡åº¦
            else:
                return (255, 165, 0)  # æ©™è‰² - ä½ç½®ä¿¡åº¦

    def close(self):
        """å…³é—­èµ„æº"""
        if self.sess:
            self.sess.close()


class FaceRecognitionApp:
    def __init__(self, root):
        self.root = root
        self.root.title("äººè„¸è¯†åˆ«ç³»ç»Ÿ - æ”¹è¿›å¸ƒå±€ç‰ˆ")
        self.root.geometry("1200x800")  # å¢å¤§çª—å£

        # åˆå§‹åŒ–å˜é‡
        self.cap = None
        self.is_running = False
        self.is_collecting = False
        self.collection_count = 0
        self.max_collection = 50
        self.current_user = "default_user"

        # åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        self.face_recognizer = ImprovedFaceRecognizer()
        
        # åˆå§‹åŒ–äººè„¸é‡‡é›†å™¨
        self.face_collector = FaceDataCollector()
        
        # åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼ˆä¸faces_my.pyä¸€è‡´ï¼‰
        try:
            self.detector = dlib.get_frontal_face_detector()
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"æ— æ³•åŠ è½½dlibäººè„¸æ£€æµ‹å™¨: {e}\nè¯·ç¡®ä¿å·²å®‰è£…dlibåº“")
            raise

        # æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„
        self.check_project_structure()

        # å°è¯•åŠ è½½æ¨¡å‹
        self.model_loaded = self.face_recognizer.load_model()

        # åˆ›å»ºç•Œé¢
        self.create_widgets()

        # ç«‹å³å¯åŠ¨æ‘„åƒå¤´
        self.start_default_camera()

    def check_project_structure(self):
        """æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„"""
        print("æ£€æŸ¥é¡¹ç›®ç›®å½•ç»“æ„...")
        dirs_to_check = ['faces_ok', 'faces_no', 'model_multi_class']
        for dir_name in dirs_to_check:
            exists = os.path.exists(dir_name)
            print(f"ç›®å½• '{dir_name}': {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")

    def start_default_camera(self):
        """å¯åŠ¨é»˜è®¤æ‘„åƒå¤´"""
        if self.cap is not None:
            self.cap.release()
        self.cap = cv2.VideoCapture(0)

        if not self.cap.isOpened():
            messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        self.is_running = True
        self.is_collecting = False  # é»˜è®¤ä¸ºè¯†åˆ«æ¨¡å¼
        self.recognize_btn.config(text="åœæ­¢è¯†åˆ«")
        self.collect_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.status_label.config(text="çŠ¶æ€: æ‘„åƒå¤´å·²å¯åŠ¨ï¼ˆè¯†åˆ«æ¨¡å¼ï¼‰")

        # æ›´æ–°ä¿¡æ¯æ 
        self.update_info("æ‘„åƒå¤´å·²å¯åŠ¨...")
        self.update_video()

    def create_widgets(self):
        # ä¸»æ¡†æ¶ - åˆ†å·¦å³ä¸¤åˆ—
        main_frame = ttk.Frame(self.root)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ (å 60%å®½åº¦)
        left_frame = ttk.Frame(main_frame, width=720)  # 1200 * 0.6 = 720
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        left_frame.pack_propagate(False)  # å›ºå®šå®½åº¦

        # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        video_frame = ttk.LabelFrame(left_frame, text="å®æ—¶ç”»é¢", padding=5)
        video_frame.pack(fill=tk.BOTH, expand=True)

        self.video_label = ttk.Label(video_frame, background="black", text="æ‘„åƒå¤´å·²å¯åŠ¨")
        self.video_label.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        # å³ä¾§ï¼šæ§åˆ¶é¢æ¿å’Œä¿¡æ¯åŒºåŸŸ (å 40%å®½åº¦)
        right_frame = ttk.Frame(main_frame, width=480)  # 1200 * 0.4 = 480
        right_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(5, 0))
        right_frame.pack_propagate(False)  # å›ºå®šå®½åº¦

        # é¡¹ç›®ä¿¡æ¯å’Œæ¨¡å‹çŠ¶æ€
        info_frame = ttk.LabelFrame(right_frame, text="é¡¹ç›®ä¿¡æ¯", padding=5)
        info_frame.pack(fill=tk.X, pady=(0, 10))

        # æ£€æŸ¥å¹¶æ˜¾ç¤ºé¡¹ç›®çŠ¶æ€
        status_text = "é¡¹ç›®çŠ¶æ€:\n"
        status_text += f"- faces_ok: {'âœ“' if os.path.exists('./faces_ok') else 'âœ—'}\n"
        status_text += f"- faces_no: {'âœ“' if os.path.exists('./faces_no') else 'âœ—'}\n"
        status_text += f"- model_multi_class: {'âœ“' if os.path.exists('./model_multi_class') else 'âœ—'}\n"

        ttk.Label(info_frame, text=status_text, justify=tk.LEFT, font=("Arial", 9)).pack(anchor=tk.W)

        # æ¨¡å‹çŠ¶æ€
        model_status_frame = ttk.LabelFrame(right_frame, text="æ¨¡å‹çŠ¶æ€", padding=5)
        model_status_frame.pack(fill=tk.X, pady=(0, 10))

        if self.model_loaded:
            status_text = f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!\n- ç±»åˆ«æ•°: {self.face_recognizer.num_classes}\n- ç±»åˆ«åç§°: {self.face_recognizer.class_names}"
            ttk.Label(model_status_frame, text=status_text, foreground="green", font=("Arial", 9)).pack(anchor=tk.W)
        else:
            status_text = "âœ— æœªæ£€æµ‹åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹\nğŸ’¡ è¯·å…ˆè¿è¡Œ faces_train_multi_person.py è®­ç»ƒæ¨¡å‹"
            ttk.Label(model_status_frame, text=status_text, foreground="red", font=("Arial", 9)).pack(anchor=tk.W)

        # æ§åˆ¶é¢æ¿
        control_frame = ttk.LabelFrame(right_frame, text="æ§åˆ¶é¢æ¿", padding=5)
        control_frame.pack(fill=tk.X, pady=(0, 10))

        # ç”¨æˆ·åè¾“å…¥
        username_frame = ttk.Frame(control_frame)
        username_frame.pack(fill=tk.X, pady=(0, 5))
        ttk.Label(username_frame, text="ç”¨æˆ·å:", font=("Arial", 9)).pack(side=tk.LEFT)
        self.user_entry = ttk.Entry(username_frame, width=15, font=("Arial", 9))
        self.user_entry.insert(0, self.current_user)
        self.user_entry.pack(side=tk.RIGHT)

        # æŒ‰é’®
        btn_frame = ttk.Frame(control_frame)
        btn_frame.pack(fill=tk.X, pady=(0, 5))

        self.collect_btn = ttk.Button(btn_frame, text="äººè„¸é‡‡é›†", command=self.toggle_collection, width=10)
        self.collect_btn.pack(side=tk.LEFT, padx=(0, 5))

        self.recognize_btn = ttk.Button(btn_frame, text="äººè„¸è¯†åˆ«", command=self.toggle_recognition, width=10)
        self.recognize_btn.pack(side=tk.LEFT, padx=(0, 5))
        if not self.model_loaded:
            self.recognize_btn.config(state=tk.DISABLED)

        self.stop_btn = ttk.Button(btn_frame, text="åœæ­¢", command=self.stop_camera, state=tk.NORMAL, width=10)
        self.stop_btn.pack(side=tk.RIGHT)

        self.status_label = ttk.Label(control_frame, text="çŠ¶æ€: æ‘„åƒå¤´å·²å¯åŠ¨", font=("Arial", 9))
        self.status_label.pack(pady=(5, 0))

        # è¿›åº¦æ¡
        self.progress = ttk.Progressbar(control_frame, mode='determinate', length=200)
        self.progress['maximum'] = self.max_collection
        self.progress.pack(pady=(5, 0), fill=tk.X)
        self.progress.pack_forget()  # é»˜è®¤éšè—

        # æ—¥å¿—ä¿¡æ¯åŒºåŸŸ
        log_frame = ttk.LabelFrame(right_frame, text="æ—¥å¿—ä¿¡æ¯", padding=5)
        log_frame.pack(fill=tk.BOTH, expand=True)

        self.info_text = scrolledtext.ScrolledText(log_frame, height=10, font=("Arial", 9))
        self.info_text.pack(fill=tk.BOTH, expand=True)

    def toggle_collection(self):
        if not self.is_running:
            self.current_user = self.user_entry.get().strip()
            if not self.current_user:
                self.current_user = "default_user"
            self.start_collection()
        else:
            self.stop_camera()

    def toggle_recognition(self):
        if not self.is_running:
            self.start_recognition()
        else:
            self.stop_camera()

    def start_collection(self):
        if self.cap is not None:
            self.cap.release()
        self.cap = cv2.VideoCapture(0)

        if not self.cap.isOpened():
            messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        self.is_running = True
        self.is_collecting = True
        self.collection_count = 0
        self.collect_btn.config(text="åœæ­¢é‡‡é›†")
        self.recognize_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.status_label.config(text="çŠ¶æ€: æ­£åœ¨é‡‡é›†äººè„¸")
        self.progress.pack(pady=(5, 0), fill=tk.X)  # æ˜¾ç¤ºè¿›åº¦æ¡

        # æ›´æ–°ä¿¡æ¯æ 
        self.update_info(f"å¼€å§‹é‡‡é›†ç”¨æˆ· '{self.current_user}' çš„äººè„¸æ•°æ®...")
        self.update_video()

    def start_recognition(self):
        if not self.model_loaded:
            messagebox.showwarning("è­¦å‘Š", "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œäººè„¸è¯†åˆ«")
            return

        if self.cap is not None:
            self.cap.release()
        self.cap = cv2.VideoCapture(0)

        if not self.cap.isOpened():
            messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return

        self.is_running = True
        self.is_collecting = False
        self.recognize_btn.config(text="åœæ­¢è¯†åˆ«")
        self.collect_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.status_label.config(text="çŠ¶æ€: æ­£åœ¨è¯†åˆ«äººè„¸")

        # æ›´æ–°ä¿¡æ¯æ 
        self.update_info("å¼€å§‹äººè„¸è¯†åˆ«...")
        self.update_video()

    def stop_camera(self):
        self.is_running = False
        if self.cap is not None:
            self.cap.release()
            self.cap = None
        self.collect_btn.config(text="äººè„¸é‡‡é›†", state=tk.NORMAL)
        self.recognize_btn.config(text="äººè„¸è¯†åˆ«", state=tk.NORMAL if self.model_loaded else tk.DISABLED)
        self.stop_btn.config(state=tk.DISABLED)
        self.status_label.config(text="çŠ¶æ€: å·²åœæ­¢")
        self.progress.pack_forget()  # éšè—è¿›åº¦æ¡

        # æ¸…ç©ºè¿›åº¦æ¡
        self.progress['value'] = 0

        # æ¸…ç©ºè§†é¢‘æ ‡ç­¾
        self.video_label.configure(image='')
        self.video_label.configure(text="æ‘„åƒå¤´å·²åœæ­¢")

        # æ›´æ–°ä¿¡æ¯æ 
        self.update_info("æ‘„åƒå¤´å·²åœæ­¢")

    def update_video(self):
        if not self.is_running:
            return

        ret, frame = self.cap.read()
        if not ret:
            self.root.after(10, self.update_video)
            return

        # æ£€æµ‹äººè„¸ï¼ˆä¸faces_my.pyä¸€è‡´ï¼‰
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.detector(gray, 1)

        for i, d in enumerate(faces):
            x1 = max(d.top(), 0)
            y1 = min(d.bottom(), frame.shape[0])
            x2 = max(d.left(), 0)
            y2 = min(d.right(), frame.shape[1])

            # ç»˜åˆ¶äººè„¸æ¡†
            if self.is_collecting:
                cv2.rectangle(frame, (x2, x1), (y2, y1), (0, 255, 0), 2)  # ç»¿è‰²æ¡†
            else:
                cv2.rectangle(frame, (x2, x1), (y2, y1), (255, 0, 0), 2)  # è“è‰²æ¡†

            if self.is_collecting:
                # é‡‡é›†æ¨¡å¼ï¼šæ£€æµ‹åˆ°äººè„¸æ—¶ä¿å­˜ç…§ç‰‡ï¼ˆä¸faces_my.pyä¸€è‡´ï¼‰
                face_img = frame[x1:y1, x2:y2]
                if face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:  # ç¡®ä¿äººè„¸è¶³å¤Ÿå¤§
                    face_resized = cv2.resize(face_img, (64, 64))

                    # ä½¿ç”¨é‡‡é›†å™¨ä¿å­˜æ•°æ®
                    success = self.face_collector.capture_data(self.current_user, self.max_collection, self.cap)
                    if success:
                        self.collection_count += 1
                        self.progress['value'] = self.collection_count

                        if self.collection_count >= self.max_collection:
                            self.stop_camera()
                            messagebox.showinfo("æç¤º",
                                                f"äººè„¸é‡‡é›†å®Œæˆï¼Œå…±é‡‡é›†{self.max_collection}å¼ ç…§ç‰‡\nä¿å­˜è‡³: ./faces_ok/{self.current_user}/")
                            break
            elif self.model_loaded:
                # è¯†åˆ«æ¨¡å¼ï¼šè¯†åˆ«äººè„¸ï¼ˆä¸face_recognition_multi_person.pyä¸€è‡´ï¼‰
                face_img = frame[x1:y1, x2:y2]
                if face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:  # ç¡®ä¿äººè„¸è¶³å¤Ÿå¤§
                    # è¯†åˆ«äººè„¸ï¼ˆä½¿ç”¨face_recognition_multi_person.pyçš„æ”¹è¿›è¯†åˆ«ï¼‰
                    predicted_class_idx, confidence, all_probs = self.face_recognizer.recognize_with_smoothing(face_img)

                    # è·å–ç±»åˆ«åç§°
                    if predicted_class_idx < len(self.face_recognizer.class_names):
                        person_name = self.face_recognizer.class_names[predicted_class_idx]
                    else:
                        person_name = "æœªçŸ¥"

                    # æ ¹æ®ç½®ä¿¡åº¦è®¾ç½®æ ‡ç­¾
                    if confidence > 0.55:
                        if person_name == "é™Œç”Ÿäºº" or "å…¶ä»–" in person_name:
                            color = (0, 0, 255)  # çº¢è‰² - é™Œç”Ÿäºº
                            label = f"é™Œç”Ÿäºº ({confidence:.2f})"
                        else:
                            color = (0, 255, 0)  # ç»¿è‰² - å·²çŸ¥äººå‘˜
                            label = f"{person_name} ({confidence:.2f})"
                    else:
                        color = (128, 128, 128)  # ç°è‰² - ç½®ä¿¡åº¦ä½
                        label = "ä½ç½®ä¿¡åº¦"

                    # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºè¯†åˆ«ç»“æœ
                    cv2.putText(frame, label, (x2, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                    # æ›´æ–°ä¿¡æ¯æ˜¾ç¤º
                    info_msg = f"è¯†åˆ«ç»“æœ: {person_name}, ç½®ä¿¡åº¦: {confidence:.3f}"
                    self.update_info(info_msg)

        # è½¬æ¢ä¸ºPILå›¾åƒå¹¶æ˜¾ç¤º
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(frame_rgb)
        img_pil = img_pil.resize((700, 500), Image.Resampling.LANCZOS)
        img_tk = ImageTk.PhotoImage(img_pil)

        self.video_label.img_tk = img_tk  # ä¿æŒå¼•ç”¨
        self.video_label.configure(image=img_tk)

        # æ¯10æ¯«ç§’æ›´æ–°ä¸€æ¬¡
        self.root.after(10, self.update_video)

    def update_info(self, message):
        """æ›´æ–°ä¿¡æ¯æ˜¾ç¤ºåŒºåŸŸ"""
        current_time = time.strftime("%H:%M:%S", time.localtime())
        self.info_text.insert(tk.END, f"[{current_time}] {message}\n")
        self.info_text.see(tk.END)

    def close_app(self):
        self.stop_camera()
        if hasattr(self, 'face_recognizer'):
            self.face_recognizer.close()
        if hasattr(self, 'cap') and self.cap is not None:
            self.cap.release()
        self.root.destroy()


def main():
    root = tk.Tk()
    app = FaceRecognitionApp(root)
    root.protocol("WM_DELETE_WINDOW", app.close_app)
    root.mainloop()


if __name__ == "__main__":
    main()



